{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705420ed-0743-4693-9741-70dea9b57e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from labml_nn.diffusion.stable_diffusion.model.autaencoder import Decoder \n",
    "from labml_nn.diffusion.stable_diffusion.model.clip_embedder import CLIPTextEmbedder \n",
    "from labml nn.diffusion.stable diffusion.model.unet import UNetModel \n",
    "from transformers import CLIPProcessor, CLIPModel \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "import loralib as lora \n",
    "from ISR.models import RDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbe4e30-daca-416d-952c-023acfc367f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionWrapper(nn.Module): \n",
    "    def _init__(self, diffusion_model: UNetModel): \n",
    "        super().__init__()\n",
    "        self.diffusion_model = diffusion_model \n",
    "    def forward(self, x: torch.Tensor, time_steps: torch.Tensor, context:torch.Tensor):\n",
    "        return self.diffusion_model(x, time_steps, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2319fa-02d9-4af3-8e5f-e45174c5f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusion(nn.Module): \n",
    "    model: Diffusionwrapper \n",
    "    decode_model: Decoder \n",
    "    text_embedding_model: CLIPTextEmbedder \n",
    "    def __init__(self, \n",
    "                 unet_model: UNetModel, \n",
    "                 decoder: decoder, \n",
    "                 clip_embedder: CLIPTextEmbedder, \n",
    "                 latent_scaling_factor: float, \n",
    "                 n_steps: int, \n",
    "                 linear_start: float, \n",
    "                 linear_end: float, \n",
    "                 ):\n",
    "        super (). init_() \n",
    "        self.model = DiffusionWrapper(unet_model) \n",
    "        self.decode_model = decoder \n",
    "        self.latent_scaling_factor = latent_scaling_factor \n",
    "        self.text_embedding_model = clip_embedder \n",
    "        self.n_steps = n_steps \n",
    "        beta = torch.linspace(linear_start**0.5, linear_end** 0.5, n_steps, dtype=torch.float64)**2 \n",
    "        self.beta = nn.Parameter(beta.to(torch.float32), requires_grad=False) \n",
    "        alpha =  1. - beta \n",
    "        alpha_bar = torch.cumprod(alpha, dim=0) \n",
    "        self.alpha_bar = nn.Parameter(alpha_bar.to(torch.float32), requires_grad=False) \n",
    "        @property \n",
    "        def device(self): \n",
    "            return next(iter(self.model.parameters())).device \n",
    "        def get_text_conditioning(self, prompts: List[str]): \n",
    "            return self.text_embedding_model(prompts) \n",
    "        def decode(self, z: torch.Tensor):\n",
    "            return self.decode_model(z/self.latent_scaling_factor)\n",
    "        def forward(self,x:torch.Tensor,t:torch.Tensor,context:torch.Tensor):\n",
    "            return self.model(x,t,context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc850620-4e28-4321-b1c3-7338c63ade13",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = CLIPMdel.from_pretrained(\"openai/clip-vit- base-patch32\") \n",
    "#processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\") \n",
    "rdn_model = RDN(weights='psnr-large', scale=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8932c2f4-aa6a-49e0-94db-9714c60e6630",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder =  Decoder(out_channels=3, \n",
    "                 z_channels=1, \n",
    "                 channels=128, \n",
    "                 channel_multipliers=[1, 2, 4, 4], \n",
    "                 n_resnet_blocks=2) \n",
    "unet_model = UNetModel(in_channels=4, \n",
    "                     out_channels=4,\n",
    "                     channels=320, \n",
    "                     attention_levels=[0, 1, 2], \n",
    "                     n_res_blocks=2, \n",
    "                     channel_multipliers=[1, 2, 4, 4], \n",
    "                     n_heads=8, \n",
    "                     tf_layers=1, \n",
    "                     d_cond=768) \n",
    "clip_embedder = CLIPTextEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a502d-5f7b-4ac2-b6fe-0077e0e7dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentDiffusion(linear_start=0.00085, \n",
    "                        linear_end=0.0120, \n",
    "                        n_steps=1000, \n",
    "                        latent_scaling_factor=0.18215,\n",
    "                        decoder=decoder, \n",
    "                        clip_embedder=clip_embedder,\n",
    "                        unet_model=unet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df46f2-4939-4d58-8288-09c9841335af",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=2e-4) \n",
    "unet_optimizer = optim.Adam(unet_model.parameters(), lr=2e-4) \n",
    "clip_ optimizer = optim.Adam(filter(lambda p: requires_grad, clip.parameters()), lr=lr=2e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c391f6-ba36-47f0-a259-b3cbee8c8c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): \n",
    "    def __init__(self, data): \n",
    "        self.data = data\n",
    "    def __len__(self): \n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx): \n",
    "        return self.data[idx]\n",
    "#Example dataset initialization '\n",
    "data = [(image_tensor1, \"a man wearing tuxedo\"), (image_tensor2, \"a girl wearing gown\")] \n",
    "dataset = CustomDataset(data) \n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a9678-99d4-4736-8f7f-d4b295a4925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([torch.exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n",
    "    return gauss / gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "def ssim(img1, img2, window_size=11, window=None, size_average=True, full=False):\n",
    "    L = 255  \n",
    "    padd = 0\n",
    "    (_, channel, height, width) = img1.size()\n",
    "    if window is None:\n",
    "        real_size = min(window_size, height, width)\n",
    "        window = create_window(real_size, channel).to(img1.device)\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1 ** 2\n",
    "    mu2_sq = mu2 ** 2\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = (0.01 * L) ** 2\n",
    "    C2 = (0.03 * L) ** 2\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "\n",
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self, window_size=11, size_average=True):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        return 1 - ssim(img1, img2, window_size=self.window_size, size_average=self.size_average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1ae58-b4fd-441a-8493-3d13dc651c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SSIMLoss(window_size=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5314ed-6736-40aa-bb6e-bd0593810caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data,num_epochs,lora_in,lora_out): \n",
    "    layer = lora.Linear(lora_in, lora_out, r=16) \n",
    "    lora.mark_only_lora_as_trainable(clip) \n",
    "    for epoch in range(num_epochs): \n",
    "        model.unet_model.train() \n",
    "        model decoder.train()\n",
    "        \n",
    "        for images, texts in dataloader:\n",
    "            \n",
    "            unet_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad() \n",
    "            clip_optimizer.zero_grad()\n",
    "\n",
    "            text_embeddings = model.clip_embedder(texts) \n",
    "             \n",
    "            with torch.no_grad(): \n",
    "                img_features clip(**text_embeddings).last_hidden_state \n",
    "            \n",
    "            latent_images = model.unet_model(img_features, text_embeddings) \n",
    "            generated_images = model.decoder(latent_images)\n",
    "            \n",
    "            loss = criterion(generated_images, images)\n",
    "            \n",
    "            loss.backward() \n",
    "            unet_optimizer.step()\n",
    "            \n",
    "            loss.backward() \n",
    "            decoder_optimizer.step()\n",
    "            \n",
    "            loss.backward() \n",
    "            clip_optimizer.step()\n",
    "         \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}],Loss: {loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc68c1-744f-4528-b469-767a1013624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(prompts): \n",
    "    text_embeddings = model.get_text_conditioning(prompts) \n",
    "    with torch.no_grad(): \n",
    "        img_features = clip(**text_embeddings).last_hidden_state \n",
    "    latent_img = model.unet_model(img_features,text_embeddings) \n",
    "    generated_images = model.decode(latent_img) \n",
    "    sr_images = rdn model.predict(generated_images)\n",
    "    return sr_images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
